{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning, FCS HSE\n",
    "\n",
    "## Practical task 4. Decomposition of error into bias and spread\n",
    "\n",
    "\n",
    "### About the task\n",
    "\n",
    "In this assignment, you will use the power of bootstrap to estimate the bias and scatter of machine learning algorithms. We will do this on the boston data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:50:28.560371Z",
     "start_time": "2021-11-17T20:50:27.563372Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:50:29.338376Z",
     "start_time": "2021-11-17T20:50:28.563372Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:50:29.370370Z",
     "start_time": "2021-11-17T20:50:29.341371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\solmi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this case special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows:\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and:\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:50:29.524372Z",
     "start_time": "2021-11-17T20:50:29.376370Z"
    }
   },
   "outputs": [],
   "source": [
    "X = boston[\"data\"]\n",
    "y = boston[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:50:29.714372Z",
     "start_time": "2021-11-17T20:50:29.526370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing bias and variance with bootstrap\n",
    "At the lecture, the following formula was derived, showing how the error of the regression algorithm can be represented as the sum of three components:\n",
    "$$\n",
    "L(\\mu) =\n",
    "    \\mathbb{E}_{x, y}\\bigl[\\mathbb{E}_{X}\\bigl[ (y - \\mu(X)(x))^2 \\bigr]\\bigr] =\n",
    "$$\n",
    "$$\n",
    "    \\underbrace{\\mathbb{E}_{x, y}\\bigl[(y - \\mathbb{E}[y|x] )^2\\bigr]}_{\\text{noise)) + \\underbrace{\\ mathbb{E}_{x}\\bigl[(\\mathbb{E}_{X}[\\mu(X)(x)] - \\mathbb{E}[y|x] )^2\\bigr]}_ {\\text{offset}} +\n",
    "    \\underbrace{\\mathbb{E}_{x}\\bigl[\\mathbb{E}_{X}\\bigl[(\\mu(X)(x) - \\mathbb{E}_{X}[\\mu( X)(x)] )^2\\bigr]\\bigr]}_{\\text{scatter)),\n",
    "$$\n",
    "* $\\mu(X)$ is an algorithm trained on the sample $X = \\{(x_1, y_1), \\dots (x_\\ell, y_\\ell)\\}$;\n",
    "* $\\mu(X)(x)$ is the response of the algorithm trained on the sample $X$ on the object $x$;\n",
    "* $\\mathbb{E}_{X}$ â€” mat. expectation over all possible samples;\n",
    "* $\\mathbb{E}_{X}[\\mu(X)(x)]$ is the \"average\" response of the algorithm trained on all possible samples $X$ on object $x$.\n",
    "    \n",
    "Using this formula, we can analyze the properties of the learning algorithm for the $\\mu$ model if we specify a probabilistic model for generating $p(x, y)$ pairs.\n",
    "\n",
    "In real problems, of course, we do not know the distribution on object pairs - the correct answer. However, we have a set of samples from this distribution (the training set) and we can use it to estimate the expected values. To evaluate mat. expectations on samples, we will use bootstrap - a method of generating \"new\" samples from one by selecting objects with a return. Let's take a look at a few steps on the way to estimating bias and scatter.\n",
    "\n",
    "#### Approximate calculation of integrals\n",
    "In the classroom, we analyzed examples of analytical calculation of bias and scatter of several learning algorithms. For most data models and learning algorithms, it will not be possible to analytically calculate mathematical expectations in formulas. However, mat. expectations can be approximated. To estimate the expectation $\\mathbb{E}_{\\bar z} f(\\bar z)$ of a function of a multidimensional random variable $\\bar z = (z_1, \\dots, z_d)$, $\\bar z \\sim p (\\bar z)$, you can generate a sample from the distribution $p(\\bar z)$ and average the value of the function over the elements of this sample:\n",
    "$$\\mathbb{E}_{\\bar z} f(z) = \\int f(\\bar z) p(\\bar z) d \\bar z \\approx \\frac 1 m \\sum_{i=1}^ m f(\\bar z_i), \\, \\bar z_i \\sim p(\\bar z), i = 1, \\dots, m.$$\n",
    "\n",
    "For example, let's estimate $\\mathbb{E}_z z^2,$ $z \\sim \\mathcal{N}(\\mu=5, \\sigma=3)$ (we know from probability theory that\n",
    "$\\mathbb{E}_z z^2 = \\sigma^2 + \\mu^2 = 34$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:50:29.904372Z",
     "start_time": "2021-11-17T20:50:29.716392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.11884393391007"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.normal(loc=5, scale=3, size=10000)\n",
    "(z**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating $\\mathbb{E}_{x, y}$\n",
    "Rate mat. the expectations on $x$ and on $x, y$ occurring in all three components of the decomposition is not difficult, because we have a sample of objects from the data distribution $p(x, y)$:\n",
    "$$ \\mathbb{E}_{x} f(x) \\approx \\frac 1 N \\sum_{i=1}^N f(x_i), \\quad\n",
    "\\mathbb{E}_{x, y} f(x, y) \\approx \\frac 1 N \\sum_{i=1}^N f(x_i, y_i),$$\n",
    "where $N$ is the number of objects in the sample, $\\{(x_i, y_i)\\}_{i=1}^N$ is the sample itself.\n",
    "\n",
    "#### Estimating $\\mathbb{E}_X$ with bootstrap\n",
    "To evaluate mat. expectation on $X$, we need a sample from the samples:\n",
    "$$\\mathbb{E}_X f(X) \\approx \\frac 1 s \\sum_{j=1}^s f(X_j),$$\n",
    "where $X_j$ is the $j$th sample. To get them, we can use bootstrap - a method of generating selections based on a selection of objects with a return. To make one sample, we will select the object index $i \\sim \\text{Uniform}(1 \\dots N)$ $N$ times and add the $i$-th pair (object, target variable) to the sample. As a result, duplicate objects may appear in each sample, and some objects may not be included in some samples at all.\n",
    "\n",
    "#### The final algorithm for estimating the bias and scatter of the $a$ algorithm\n",
    "1. Generate $s$ samples $X_j$ using the bootstrap method.\n",
    "1. On each sample $X_j$, train the algorithm $a_j$.\n",
    "1. For each sample $X_j$ determine the set of objects $T_j$ that are not included in it (out-of-bag). Calculate predictions of $a_j$ algorithm on $T_j$ objects.\n",
    "\n",
    "Since we only have one answer for each object, we will assume that the noise is 0 and $\\mathbb{E}[y|x]$ is equal to the correct answer for the object $x$.\n",
    "\n",
    "Final grades:\n",
    "* Offset: for one object, the square of the difference between the average prediction and the correct answer. The average prediction is taken only for those algorithms $a_j$ for which this object was included in the out-of-bag sample $T_j$. To obtain the total displacement, average the displacements over the objects.\n",
    "* Scatter: for one object - the sample variance of $a_j$ algorithm predictions for which this object was included in the $T_j$ out-of-bag sample. To obtain a general scatter, average the scatter over objects.\n",
    "* Error $L$: average the squared differences between the prediction and the correct answer over all predictions made for all objects.\n",
    "\n",
    "As a result, it should turn out that the error is approximately equal to the sum of the bias and spread!\n",
    "\n",
    "The algorithm is also briefly described at [link](https://web.engr.oregonstate.edu/~tgd/classes/534/slides/part9.pdf) (slides 19-21).\n",
    "\n",
    "__one. (3 points)__\n",
    "\n",
    "Implement the described algorithm. Please note that if the object is not included in any of the out-of-bag selections, you do not need to take it into account in calculating the total values. As usual, only one run is allowed - over samples (from 0 to num_runs-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:50:30.144371Z",
     "start_time": "2021-11-17T20:50:29.907372Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_biase_variance(regressor, X, y, num_runs=1000):\n",
    "    \"\"\"\n",
    "    :param regressor: sklearn estimator with fit(...) and predict(...) method\n",
    "    :param X: numpy-array representing training set ob objects, shape [n_obj, n_feat]\n",
    "    :param y: numpy-array representing target for training objects, shape [n_obj]\n",
    "    :param num_runs: int, number of samples (s in the description of the algorithm)\n",
    "    \n",
    "    :returns: bias (float), variance (float), error (float) \n",
    "    each value is computed using bootstrap\n",
    "    \"\"\"\n",
    "    ind = np.random.randint(0, len(X), len(X)*num_runs).reshape(num_runs, -1)\n",
    "    al = [x for x in range(len(X))]\n",
    "    df = pd.DataFrame(columns=[0,1])\n",
    "    error_all = np.array([])\n",
    "    \n",
    "    for i in ind:\n",
    "        regressor.fit(X[i], y[i])\n",
    "        ind_test = list(set(al) - set(i))\n",
    "        y_pred = regressor.predict(X[ind_test])\n",
    "        df = df.append(pd.DataFrame(list(zip(ind_test, y_pred))))\n",
    "        error_all = np.append(error_all, (y[ind_test] - y_pred)**2)\n",
    "    \n",
    "    df.set_index(np.array([x for x in range(len(df))]), inplace = True)\n",
    "    bias = np.mean((np.array(df.groupby(0).mean()).reshape(len(y),) - y)**2)\n",
    "    variance = np.mean((np.array(df.groupby(0).std()))**2)\n",
    "    error = error_all.mean()\n",
    "    return bias, variance, error    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. (0 points)__\n",
    "\n",
    "Estimate bias, scatter, and error for three algorithms with default hyperparameters: linear regression, decision tree, and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T21:02:07.295668Z",
     "start_time": "2021-11-17T20:50:30.148370Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(): bias = 23.767655069714465; variance = 0.9554026796952855; mse = 24.803334108748416\n",
      "DecisionTreeRegressor(): bias = 10.1140784639303; variance = 13.068350385077718; mse = 23.111529356571257\n",
      "RandomForestRegressor(): bias = 10.75976069652386; variance = 2.2840408944566297; mse = 13.143811678552384\n",
      "GradientBoostingRegressor(): bias = 9.210643476074777; variance = 2.031754526234437; mse = 11.223787739015004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regressor = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor()]\n",
    "\n",
    "for regressor in regressor:\n",
    "    bias, variance, error = compute_biase_variance(regressor, X, y)\n",
    "    print('{}: bias = {}; variance = {}; mse = {}'.format(regressor, bias, variance, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T20:30:13.334047Z",
     "start_time": "2021-11-17T20:28:13.417419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(): bias = 32.43604736455349; variance = 1.6408175036848205; mse = 11.037919652173743\n",
      "DecisionTreeRegressor(): bias = 16.53716414215689; variance = 16.079058406862746; mse = 11.037919652173743\n",
      "RandomForestRegressor(): bias = 16.928774081232838; variance = 3.1936187681789248; mse = 11.037919652173743\n",
      "GradientBoostingRegressor(): bias = 13.356529621428038; variance = 3.414108077099948; mse = 11.037919652173743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "regressor = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor()]\n",
    "\n",
    "for regressor in regressor:\n",
    "    mse, bias, variance = bias_variance_decomp(regressor, X_train, y_train, X_test, y_test, loss='mse', num_rounds=200, random_seed=1)\n",
    "    print('{}: bias = {}; variance = {}; mse = {}'.format(regressor, bias, variance, error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
